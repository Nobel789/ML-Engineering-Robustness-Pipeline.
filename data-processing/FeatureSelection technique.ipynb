{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Two predictive models enter the testing phase. One is bogged down with irrelevant features, performs poorly, and misses key insights. The other uses only the most critical features, runs efficiently, and delivers sharp predictions. What made the difference? In this video, we'll explore feature selection techniques like backward elimination, forward selection, and lasso to ensure your model is on the winning side."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start with backward elimination, a step-by-step process where we begin with all features and gradually remove those that are not statistically significant based on their p-values, a measure of central tendency. Knowledge of this value permits a statistician or model to accept or reject a data point based on the likelihood that it is statistically significant."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install statsmodels"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: statsmodels in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.14.6)\nRequirement already satisfied: numpy<3,>=1.22.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from statsmodels) (1.23.5)\nRequirement already satisfied: scipy!=1.9.2,>=1.8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from statsmodels) (1.15.3)\nRequirement already satisfied: pandas!=2.1.0,>=1.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from statsmodels) (1.5.3)\nRequirement already satisfied: patsy>=0.5.6 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from statsmodels) (1.0.2)\nRequirement already satisfied: packaging>=21.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from statsmodels) (25.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create 50 rows of data instead of 10\n",
        "data = {\n",
        "    'StudyHours': np.random.randint(1, 10, 50),\n",
        "    'PrevExamScore': np.random.randint(30, 100, 50),\n",
        "    'Pass': np.random.randint(0, 2, 50)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Now when you run your model, the warning will vanish!"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1765029303380
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Features and target variable\n",
        "X = df[['StudyHours', 'PrevExamScore']]\n",
        "y = df['Pass']\n",
        "\n",
        "# Add constant (intercept) to the model\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the OLS model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Display model summary with p-values\n",
        "print(model.summary())\n",
        "\n",
        "# Remove the least significant feature (highest p-value)\n",
        "if model.pvalues['StudyHours'] > 0.05:\n",
        "    X = X.drop(columns='StudyHours')\n",
        "    model = sm.OLS(y, X).fit()\n",
        "\n",
        "print(model.summary())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   Pass   R-squared:                       0.092\nModel:                            OLS   Adj. R-squared:                  0.053\nMethod:                 Least Squares   F-statistic:                     2.380\nDate:                Sat, 06 Dec 2025   Prob (F-statistic):              0.104\nTime:                        13:55:47   Log-Likelihood:                -32.857\nNo. Observations:                  50   AIC:                             71.71\nDf Residuals:                      47   BIC:                             77.45\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst             0.9131      0.296      3.083      0.003       0.317       1.509\nStudyHours        0.0277      0.028      0.995      0.325      -0.028       0.084\nPrevExamScore    -0.0065      0.003     -1.883      0.066      -0.014       0.000\n==============================================================================\nOmnibus:                       64.274   Durbin-Watson:                   2.337\nProb(Omnibus):                  0.000   Jarque-Bera (JB):                6.145\nSkew:                          -0.340   Prob(JB):                       0.0463\nKurtosis:                       1.423   Cond. No.                         317.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   Pass   R-squared:                       0.073\nModel:                            OLS   Adj. R-squared:                  0.054\nMethod:                 Least Squares   F-statistic:                     3.771\nDate:                Sat, 06 Dec 2025   Prob (F-statistic):             0.0580\nTime:                        13:55:47   Log-Likelihood:                -33.378\nNo. Observations:                  50   AIC:                             70.76\nDf Residuals:                      48   BIC:                             74.58\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst             1.0693      0.251      4.259      0.000       0.564       1.574\nPrevExamScore    -0.0067      0.003     -1.942      0.058      -0.014       0.000\n==============================================================================\nOmnibus:                       67.842   Durbin-Watson:                   2.413\nProb(Omnibus):                  0.000   Jarque-Bera (JB):                6.267\nSkew:                          -0.350   Prob(JB):                       0.0436\nKurtosis:                       1.414   Cond. No.                         267.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1765029348372
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward selection"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def forward_selection(X, y):\n",
        "    remaining_features = set(X.columns)\n",
        "    selected_features = []\n",
        "    current_score = 0.0\n",
        "    \n",
        "    while remaining_features:\n",
        "        scores_with_candidates = []\n",
        "        \n",
        "        for feature in remaining_features:\n",
        "            features_to_test = selected_features + [feature]\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X[features_to_test], y, test_size=0.2, random_state=42)\n",
        "            \n",
        "            # Train the model\n",
        "            model = LinearRegression()\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            score = r2_score(y_test, y_pred)\n",
        "            \n",
        "            scores_with_candidates.append((score, feature))\n",
        "        \n",
        "        # Select the feature with the best score\n",
        "        scores_with_candidates.sort(reverse=True)\n",
        "        best_score, best_feature = scores_with_candidates[0]\n",
        "        \n",
        "        if current_score < best_score:\n",
        "            remaining_features.remove(best_feature)\n",
        "            selected_features.append(best_feature)\n",
        "            current_score = best_score\n",
        "        else:\n",
        "            break\n",
        "    \n",
        "    return selected_features\n",
        "\n",
        "best_features = forward_selection(X, y)\n",
        "print(f\"Selected features: {best_features}\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Selected features: []\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1765029474709
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the LASSO model with a regularization strength (alpha)\n",
        "lasso_model = Lasso(alpha=0.1)\n",
        "\n",
        "# Train the LASSO model\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = lasso_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f'R-squared score: {r2}')\n",
        "\n",
        "# Show the coefficients\n",
        "print(f'LASSO Coefficients: {lasso_model.coef_}')\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "R-squared score: -0.00841181729560092\nLASSO Coefficients: [ 0.         -0.00727535]\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1765029555324
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This way, LASSO selects important features while maintaining model simplicity. To wrap up, remember these key points. Using feature selection techniques like backward elimination, forward selection, and LASSO helps you focus on the most important features, improving your model's performance. Streamlining your dataset reduces complexity and prevents overfitting, making your model both efficient and accurate. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}