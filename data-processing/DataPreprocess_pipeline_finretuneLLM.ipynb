{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing datasets is a critical step in ensuring high-quality input for machine learning models. Cleaning and organizing the data, tokenizing text, and handling missing values are essential for preparing data to be used in fine-tuning tasks. This walkthrough will guide you through the steps needed to preprocess raw text data and structure it for a fine-tuning task.\n",
        "\n",
        "By the end of this walkthrough, you will be able to:\n",
        "\n",
        "    Clean and preprocess raw text data for machine learning tasks.\n",
        "\n",
        "    Apply tokenization and text normalization techniques.\n",
        "\n",
        "    Prepare your dataset for fine-tuning in a structured way."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    Step 1: Data Preprocessing\n",
        "\n",
        "    Step 2: Clean the text \n",
        "\n",
        "    Step 3: Tokenize\n",
        "\n",
        "    Step 4: Handle missing data\n",
        "\n",
        "    Step 5: Prepare the data for fine-tuning\n",
        "\n",
        "    Step 6: Split the data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Data Preprocessing\n",
        "\n",
        " Before diving into the cleaning and tokenization processes, it's essential to import and organize the raw data into a structured format. We begin by loading the dataset, defining necessary labels, and preparing the initial dataset.  "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "data = pd.read_csv('customer_data.csv')\n",
        "\n",
        "# Define mapping for labels\n",
        "label_mapping = {'Bronze': 0, 'Silver': 1, 'Gold': 2}  # Assign numbers to each category\n",
        "\n",
        "# Convert membership_level to numeric labels\n",
        "data['label'] = data['membership_level'].map(label_mapping)\n",
        "\n",
        "# Convert labels to PyTorch tensor\n",
        "labels = torch.tensor(data['label'].tolist())\n",
        "data['text'] = [\"Hello, I am a Bronze member!\", \n",
        "                        \"Silver membership offers perks.\", \n",
        "                        \"Gold members get premium benefits.\", \n",
        "                        \"Silver members enjoy discounts.\", \n",
        "                        \"Bronze is the starting tier.\"]"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1764849895606
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Clean the text \n",
        "\n",
        "Text cleaning is the first step in preparing your dataset. It involves removing unwanted characters, URLs, and excess whitespace to ensure uniformity and cleanliness in the data. Text is also changed to lowercase to maintain consistency across all data points."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to clean the text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Apply cleaning function to your dataset\n",
        "data['cleaned_text'] = data['text'].apply(clean_text)\n",
        "print(data['cleaned_text'].head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0           hello i am a bronze member\n1       silver membership offers perks\n2    gold members get premium benefits\n3       silver members enjoy discounts\n4          bronze is the starting tier\nName: cleaned_text, dtype: object\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1764849899150
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "Cleaning the text by removing unnecessary characters and formatting it ensures that the data is consistent, making it easier for the model to understand."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Tokenize\n",
        "\n",
        "Tokenization is the process of converting text into individual tokens that a machine-learning model can understand. We use the tokenizer corresponding to the pretrained model (e.g., BERT) for this. This ensures that the data is properly formatted and ready for fine-tuning"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the cleaned text\n",
        "tokens = tokenizer(\n",
        "    data['cleaned_text'].tolist(), padding=True, truncation=True, return_tensors='pt', max_length=128\n",
        ")\n",
        "\n",
        "print(tokens['input_ids'][:5])  # Check the first 5 tokenized examples"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([[  101,  7592,  1045,  2572,  1037,  4421,  2266,   102],\n        [  101,  3165,  5779,  4107,  2566,  5705,   102,     0],\n        [  101,  2751,  2372,  2131, 12882,  6666,   102,     0],\n        [  101,  3165,  2372,  5959, 19575,  2015,   102,     0],\n        [  101,  4421,  2003,  1996,  3225,  7563,   102,     0]])\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1764849997569
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "Tokenization converts the cleaned text into a format suitable for fine-tuning the model, ensuring that the input is ready for training."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Handle missing data\n",
        "\n",
        "Missing data is common in real-world datasets. You can handle it either by removing incomplete entries or by imputing missing values. This step is critical to preventing errors during the training process."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing data\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Option 1: Drop rows with missing data\n",
        "data = data.dropna()\n",
        "\n",
        "# Option 2: Fill missing values with a placeholder\n",
        "data['cleaned_text'].fillna('missing', inplace=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "customer_id         0\nmembership_level    0\nlabel               0\ntext                0\ncleaned_text        0\ndtype: int64\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1764850143542
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "Handling missing data ensures that your dataset is complete, which prevents training interruptions or biases introduced by missing information."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Prepare the data for fine-tuning\n",
        "\n",
        "After cleaning and tokenizing your text, the next step is to prepare the data for fine-tuning. This involves structuring the tokenized data and labels into a format suitable for training, such as PyTorch DataLoader objects"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create PyTorch tensors from the tokenized data\n",
        "input_ids = tokens['input_ids']\n",
        "attention_masks = tokens['attention_mask']\n",
        "labels = torch.tensor(data['label'].tolist())\n",
        "\n",
        "# Create a DataLoader for training\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "print(\"DataLoader created successfully!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "DataLoader created successfully!\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1764850212647
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "Organizing your data into DataLoader objects is necessary for model training, allowing the model to process the data in batches efficiently."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Split the data\n",
        "\n",
        "Before training, it’s important to split your data into training, validation, and test sets. The training set is used to train the model, the validation set helps to tune model hyperparameters, and the test set is used for final evaluation to ensure that the model generalizes well to unseen data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First, split data into a combined training + validation set and a test set\n",
        "train_val_inputs, test_inputs, train_val_labels, test_labels = train_test_split(\n",
        "    input_ids, labels, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Now, split the combined set into training and validation sets\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
        "    train_val_inputs, train_val_labels, test_size=0.15, random_state=42\n",
        ")\n",
        "\n",
        "# Create DataLoader objects for training, validation, and test sets\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "val_dataset = TensorDataset(val_inputs, val_labels)\n",
        "test_dataset = TensorDataset(test_inputs, test_labels)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1764850343198
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "The train_test_split method from the sklearn.model_selection module splits your data into training and validation (or test) sets. Here's a breakdown of how it works:\n",
        "\n",
        "    input_ids and labels: these are the inputs and labels you are splitting.\n",
        "\n",
        "    test_size=0.1: this indicates that 10 percent of the data will be set aside for the test set.\n",
        "\n",
        "    random_state=42: this ensures the split is reproducible—using the same random state will produce the same split every time.\n",
        "\n",
        "In this case, we first split the data into two sets:\n",
        "\n",
        "    train_val_inputs and test_inputs: a combined set of training + validation data and a test set.\n",
        "\n",
        "    Then, we further split the train_val_inputs into train_inputs and val_inputs to get a separate validation set."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}