{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Think of an AI/ML system like a car engine.\n",
        "If one small part breaks‚Äîlike a loose bolt‚Äîthe entire car can stop running.\n",
        "Similarly, in AI systems, even a tiny mistake (wrong data, bad input, or a training issue) can make the whole model fail.\n",
        "\n",
        "To keep the ‚ÄúAI engine‚Äù running smoothly, we need error handling, which works like:\n",
        "\n",
        "Seatbelts ‚Üí protect the system from crashing (input validation)\n",
        "\n",
        "Warning lights ‚Üí tell us when something goes wrong (error logging)\n",
        "\n",
        "Shock absorbers ‚Üí help the system recover instead of breaking (exception handling)\n",
        "\n",
        "With good error handling, AI systems become safer, more reliable, and easier to fix.\n",
        "\n",
        "By the end of this section, you will learn how to:\n",
        "\n",
        "Find common errors in machine learning pipelines\n",
        "\n",
        "Validate inputs to avoid bad data\n",
        "\n",
        "Catch and handle errors during model training\n",
        "\n",
        "Log errors so you know what went wrong\n",
        "\n",
        "Test your system to make sure it handles problems correctly\n",
        "\n",
        "In short: error handling makes your ML system tougher, smarter, and more reliable‚Äîjust like safety systems do for a car.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and save a toy dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set seed so the random data is the same every run\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a toy dataset with 100 rows\n",
        "data = {\n",
        "    'feature1': np.random.choice(['error_A', 'error_B', 'error_C'], size=100),\n",
        "    'feature2': np.random.choice(['severity_high', 'severity_low'], size=100),\n",
        "    'solution': np.random.choice(['restart', 'update', 'contact_support'], size=100)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save dataset to a CSV file\n",
        "df.to_csv('toy_data.csv', index=False)\n",
        "\n",
        "print(\"Toy dataset created and saved as toy_data.csv\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Toy dataset created and saved as toy_data.csv\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1765014359189
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load & Explore the Dataset\n",
        "\n",
        "This is like opening the file in Excel and checking the first few rows."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('toy_data.csv')\n",
        "\n",
        "# Explore the dataset\n",
        "print(df.info())     # shows column names, data types, row count\n",
        "print(df.head())     # shows the first 5 rows\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100 entries, 0 to 99\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   feature1  100 non-null    object\n 1   feature2  100 non-null    object\n 2   solution  100 non-null    object\ndtypes: object(3)\nmemory usage: 2.5+ KB\nNone\n  feature1       feature2         solution\n0  error_C   severity_low          restart\n1  error_A   severity_low  contact_support\n2  error_C   severity_low           update\n3  error_C  severity_high          restart\n4  error_A  severity_high          restart\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1765014403591
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Validation With Error Handling\n",
        "\n",
        "Think of this like a quality check:\n",
        "\n",
        "‚ùå Is the input NOT a DataFrame? ‚Üí Error\n",
        "\n",
        "‚ùå Are there missing values? ‚Üí Error\n",
        "\n",
        "‚úîÔ∏è Otherwise ‚Üí It‚Äôs valid"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_data(data):\n",
        "    try:\n",
        "        if not isinstance(data, pd.DataFrame):\n",
        "            raise ValueError(\"Input must be a pandas DataFrame.\")\n",
        "        \n",
        "        if data.isnull().values.any():\n",
        "            raise ValueError(\"Missing values detected in the dataset.\")\n",
        "        \n",
        "        print(\"Data validation successful.\")\n",
        "    \n",
        "    except ValueError as e:\n",
        "        print(f\"Data validation error: {e}\")\n",
        "\n",
        "# Validate the dataset\n",
        "validate_data(df)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Data validation successful.\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1765014677341
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Split Data & Train a Model With Error Handling\n",
        "üëâ We split data into features (X) and solution labels (y):\n",
        "\n",
        "X = the symptoms\n",
        "\n",
        "y = the predicted solution\n",
        "\n",
        "Then we train a Decision Tree model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert text into numbers using one-hot encoding\n",
        "X = pd.get_dummies(df[['feature1', 'feature2']])\n",
        "y = df['solution']\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1765014730669
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training with error handling"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def train_model(X_train, y_train):\n",
        "    try:\n",
        "        model = DecisionTreeClassifier()\n",
        "        model.fit(X_train, y_train)\n",
        "        print(\"Model trained successfully.\")\n",
        "        return model\n",
        "    except ValueError as e:\n",
        "        print(f\"Model training error: {e}\")\n",
        "\n",
        "# Train the model\n",
        "model = train_model(X_train, y_train)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model trained successfully.\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1765014799553
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Error Logging\n",
        "\n",
        "Instead of just printing errors, we also write them to a file.\n",
        "\n",
        "Think of this like a black box recorder in an airplane."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# Set up logging to file\n",
        "logging.basicConfig(filename='ml_errors.log', level=logging.ERROR)\n",
        "\n",
        "def validate_data_with_logging(data):\n",
        "    try:\n",
        "        if not isinstance(data, pd.DataFrame):\n",
        "            raise ValueError(\"Input must be a pandas DataFrame.\")\n",
        "        \n",
        "        if data.isnull().values.any():\n",
        "            raise ValueError(\"Missing values detected in the dataset.\")\n",
        "        \n",
        "        print(\"Data validation successful.\")\n",
        "    \n",
        "    except ValueError as e:\n",
        "        logging.error(f\"Data validation error: {e}\")\n",
        "        print(f\"Logged error: {e}\")\n",
        "\n",
        "# Validate normally\n",
        "validate_data_with_logging(df)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Data validation successful.\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1765015001060
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "test error handling"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a damaged dataset with missing values\n",
        "df_with_missing = df.copy()\n",
        "df_with_missing.iloc[0, 0] = None  # remove one value\n",
        "\n",
        "# Validate again\n",
        "validate_data_with_logging(df_with_missing)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Logged error: Missing values detected in the dataset.\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1765015007417
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "bert-py310-cpu",
      "display_name": "Python 3.10 - bert-py310-cpu"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.19",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}